# -*- coding: utf-8 -*-
"""Submission1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V77vRN4hhAuYrEbv7wSGZzJsxHgtXLOL
"""

import pandas as pd
import numpy as np
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten, LSTM
from scipy.stats import skew
import string 
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer


df = pd.read_csv('/content/phone_reviews.csv')
df.head()

print(len(df))

df.isnull().sum()

sns.set_style('darkgrid')
sns.countplot(df.star)

df = df.drop(columns=['Unnamed: 0','mobile_names','asin','title'])
df.head()

string.punctuation

def remove_punct(text):
  text_nopunct = "".join([char for char in text if char not in string.punctuation])
  return text_nopunct
df['body_clean'] = df['body'].apply(lambda x: remove_punct(x))
df.head()

def tokenize(text):
  tokens = re.split("[\s.,!?:;'\"-]+{}=()",text)
  return tokens

df['body_tokenized'] = df['body_clean'].apply(lambda x: tokenize(x.lower()))
df.head()

nltk.download('stopwords')

stopword = nltk.corpus.stopwords.words('english')

def remove_stopwords(tokenized_list):
  text = [word for word in tokenized_list if word not in stopword]
  return text

df['body_x'] = df['body_tokenized'].apply(lambda x: remove_stopwords(x))
df.head()

ps = nltk.PorterStemmer()

def stemming(tokenized_text):
  text = [ps.stem(word) for word in tokenized_text]
  return text

df['body_stemmed'] =  df['body_x'].apply(lambda x: stemming(x))
df.head()

nltk.download('wordnet')
wn = nltk.WordNetLemmatizer()

def lemmatizing(tokenized_text):
  text = [wn.lemmatize(word) for word in tokenized_text]
  return text

df['body_lemmatized'] =  df['body_x'].apply(lambda x: lemmatizing(x))
df.head()

df['body'] = df['body_clean']
df.head()

df.isnull().values.any()

category = pd.get_dummies(df.star)
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='star')
df_baru.head()

deskripsi = df_baru['body'].values
label = df_baru[[1,2,3,4,5]].values

deskripsi_latih, deskripsi_test, label_latih, label_test = train_test_split(deskripsi, label, test_size=0.2)

tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(deskripsi_latih)
tokenizer.fit_on_texts(deskripsi_test)
 
sekuens_latih = tokenizer.texts_to_sequences(deskripsi_latih)
sekuens_test = tokenizer.texts_to_sequences(deskripsi_test)
 
padded_latih = pad_sequences(sekuens_latih) 
padded_test = pad_sequences(sekuens_test)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(128, activation='relu', input_shape=(6,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(5, activation='softmax')
])

model.compile(loss='categorical_crossentropy', 
              optimizer='adam',
              metrics=['accuracy'])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
     if(logs.get('accuracy') > 0.90 and logs.get('val_accuracy') > 0.90):
      print('\n Akurasi telah mencapai > 90%')
      self.model_stop_training = True
callbacks = myCallback()

hist = model.fit(padded_latih, label_latih, epochs=5, 
                 validation_data=(padded_test, label_test),
                 validation_split=0.2,
                 verbose=2,
                 batch_size=64,
                 callbacks=[callbacks])

plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train','Validation'], loc='bottom right')
plt.show()

plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train','Validation'], loc='upper right')
plt.show()